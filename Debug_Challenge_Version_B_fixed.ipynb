{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Advanced Debug Challenge - Team Model (HARD MODE \ud83d\udd25)\n", "## Interactive Activity: Find and Fix ALL the Bugs!\n", "\n", "**Difficulty Level: ADVANCED**\n", "\n", "This model has **MULTIPLE BUGS** that work together to break training in subtle ways. Some bugs are obvious, others are sneaky!\n", "\n", "**Your Task**: \n", "1. Run all the cells\n", "2. Observe what happens (take notes!)\n", "3. Find ALL the bugs (there are 5)\n", "4. Fix them systematically\n", "5. Explain each bug and its impact\n", "\n", "**Hints**: \n", "- Some bugs are in the math/logic\n", "- Some bugs are in data handling\n", "- Some bugs only appear after several iterations\n", "- The bugs interact with each other!\n", "- Print intermediate values to debug\n", "\n", "**Scoring:**\n", "- Find 1-2 bugs: Beginner debugging skills\n", "- Find 3 bugs: Good debugging skills\n", "- Find 4 bugs: Excellent debugging skills  \n", "- Find ALL 5 bugs: Master debugger! \ud83c\udfc6\n", "\n", "Good luck - you'll need it! \ud83d\udc1b\ud83d\udd0d"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "np.random.seed(42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load Dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create synthetic binary classification dataset\n", "m_train = 200\n", "m_test = 50\n", "n_features = 20\n", "\n", "# Generate data with some structure (not purely random)\n", "np.random.seed(42)\n", "\n", "# Training data\n", "X_train = np.random.randn(n_features, m_train)\n", "true_weights = np.random.randn(n_features, 1) * 0.5\n", "y_train_logits = np.dot(true_weights.T, X_train) + 0.5\n", "y_train = (y_train_logits > 0).astype(int)\n", "\n", "# Test data\n", "X_test = np.random.randn(n_features, m_test)\n", "y_test_logits = np.dot(true_weights.T, X_test) + 0.5\n", "y_test = (y_test_logits > 0).astype(int)\n", "\n", "print(f\"Training set: X_train.shape = {X_train.shape}, y_train.shape = {y_train.shape}\")\n", "print(f\"Test set: X_test.shape = {X_test.shape}, y_test.shape = {y_test.shape}\")\n", "print(f\"Training labels - 0s: {np.sum(y_train == 0)}, 1s: {np.sum(y_train == 1)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Helper Functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sigmoid(z):\n", "    \"\"\"\n", "    Compute sigmoid activation\n", "    \"\"\"\n", "    # \ud83d\udc1b BUG #1: Numerical instability with large values\n", "    # This can cause overflow/underflow issues\n", "    z = np.clip(z, -500, 500)\n", "    return 1 / (1 + np.exp(-z))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def initialize_parameters(n_features):\n", "    \"\"\"\n", "    Initialize weights and bias\n", "    \"\"\"\n", "    # \ud83d\udc1b BUG #2: Bad initialization - weights too large!\n", "    # This causes gradients to explode\n", "    w = np.random.randn(n_features, 1) * 0.01  # Should be small, not *10!\n", "    b = 0.0\n", "    return w, b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def propagate(w, b, X, y):\n", "    \"\"\"\n", "    Forward and backward propagation\n", "    \"\"\"\n", "    m = X.shape[1]\n", "    \n", "    # Forward propagation\n", "    A = sigmoid(np.dot(w.T, X) + b)\n", "    \n", "    # \ud83d\udc1b BUG #3: Wrong cost calculation!\n", "    # Missing the negative sign in cross-entropy\n", "    # Should be: cost = -1/m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n", "    epsilon = 1e-8\n", "    cost = -1/m * np.sum(y*np.log(A+epsilon) + (1-y)*np.log(1-A+epsilon))\n", "    \n", "    # \ud83d\udc1b BUG #4: Wrong gradient calculation!\n", "    # Signs are flipped - should be (A - y), not (y - A)\n", "    dw = 1/m * np.dot(X, (A - y).T)  # WRONG SIGN!\n", "    db = 1/m * np.sum(A - y)  # WRONG SIGN!\n", "    \n", "    grads = {\"dw\": dw, \"db\": db}\n", "    return grads, cost"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training Function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(w, b, X, y, num_iterations, learning_rate, print_cost=True):\n", "    \"\"\"\n", "    Optimize parameters using gradient descent\n", "    \"\"\"\n", "    costs = []\n", "    \n", "    for i in range(num_iterations):\n", "        # Get gradients and cost\n", "        grads, cost = propagate(w, b, X, y)\n", "        \n", "        dw = grads[\"dw\"]\n", "        db = grads[\"db\"]\n", "        \n", "        # Update parameters\n", "        w = w - learning_rate * dw\n", "        b = b - learning_rate * db\n", "        \n", "        # Record cost\n", "        if i % 100 == 0:\n", "            costs.append(cost)\n", "            if print_cost:\n", "                print(f\"Cost after iteration {i}: {cost:.6f}\")\n", "    \n", "    params = {\"w\": w, \"b\": b}\n", "    return params, costs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict(w, b, X):\n", "    \"\"\"\n", "    Predict labels for dataset X\n", "    \"\"\"\n", "    m = X.shape[1]\n", "    y_pred = np.zeros((1, m))\n", "    \n", "    A = sigmoid(np.dot(w.T, X) + b)\n", "    \n", "    # \ud83d\udc1b BUG #5: Wrong prediction threshold and logic!\n", "    # Should be: if A[0, i] > 0.5: y_pred[0, i] = 1\n", "    # But this uses <= instead of >\n", "    for i in range(A.shape[1]):\n", "        if A[0, i] > 0.5:  # WRONG! Should be > 0.5\n", "            y_pred[0, i] = 1\n", "        else:\n", "            y_pred[0, i] = 0\n", "    \n", "    return y_pred"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Run the Model\n", "\n", "Watch what happens when you run this... \ud83d\udd25"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize parameters\n", "w, b = initialize_parameters(n_features)\n", "\n", "print(f\"Initial w stats: mean={np.mean(w):.3f}, std={np.std(w):.3f}, max={np.max(np.abs(w)):.3f}\")\n", "print(f\"Initial b: {b}\")\n", "print()\n", "\n", "# Set learning rate\n", "learning_rate = 0.01\n", "num_iterations = 2000\n", "\n", "# Train the model\n", "print(\"Training the model...\\n\")\n", "try:\n", "    params, costs = train(w, b, X_train, y_train, num_iterations, learning_rate)\n", "    print(\"\\n\u2705 Training completed!\")\n", "except Exception as e:\n", "    print(f\"\\n\u274c Training failed with error: {e}\")\n", "    print(\"\\nThis is expected with these bugs. Try to fix them!\")\n", "    params = {\"w\": w, \"b\": b}\n", "    costs = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualize Results (if training completed)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if len(costs) > 0:\n", "    # Plot learning curve\n", "    plt.figure(figsize=(12, 4))\n", "    \n", "    # Cost curve\n", "    plt.subplot(1, 2, 1)\n", "    plt.plot(costs)\n", "    plt.ylabel('Cost')\n", "    plt.xlabel('Iterations (hundreds)')\n", "    plt.title('Learning Curve - Does this look right?')\n", "    plt.grid(True)\n", "    \n", "    # Cost change rate\n", "    plt.subplot(1, 2, 2)\n", "    if len(costs) > 1:\n", "        cost_changes = np.diff(costs)\n", "        plt.plot(cost_changes)\n", "        plt.ylabel('Cost Change')\n", "        plt.xlabel('Iterations (hundreds)')\n", "        plt.title('Cost Change Rate - Should decrease!')\n", "        plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n", "        plt.grid(True)\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "    \n", "    # Print diagnostics\n", "    print(\"\\n\ud83d\udcca DIAGNOSTICS:\")\n", "    print(f\"Starting cost: {costs[0]:.6f}\")\n", "    print(f\"Final cost: {costs[-1]:.6f}\")\n", "    print(f\"Cost change: {costs[-1] - costs[0]:.6f}\")\n", "    print(f\"Cost should DECREASE (be negative). Is it? {costs[-1] < costs[0]}\")\n", "    \n", "    if len(costs) > 1:\n", "        increasing = sum(1 for i in range(len(costs)-1) if costs[i+1] > costs[i])\n", "        print(f\"\\nIterations where cost INCREASED: {increasing}/{len(costs)-1}\")\n", "        print(\"\ud83d\udea8 This should be near 0 for working gradient descent!\")\n", "else:\n", "    print(\"\u274c No costs to plot - training failed immediately!\")\n", "    print(\"Check for errors like division by zero, NaN, or overflow.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Evaluate Performance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    # Get predictions\n", "    y_pred_train = predict(params[\"w\"], params[\"b\"], X_train)\n", "    y_pred_test = predict(params[\"w\"], params[\"b\"], X_test)\n", "    \n", "    # Calculate accuracy\n", "    train_accuracy = 100 - np.mean(np.abs(y_pred_train - y_train)) * 100\n", "    test_accuracy = 100 - np.mean(np.abs(y_pred_test - y_test)) * 100\n", "    \n", "    print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n", "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n", "    \n", "    # Sanity check\n", "    print(\"\\n\ud83d\udd0d SANITY CHECKS:\")\n", "    print(f\"Baseline (random guessing): ~50%\")\n", "    print(f\"Your model: {test_accuracy:.2f}%\")\n", "    \n", "    if test_accuracy < 55:\n", "        print(\"\ud83d\udea8 Model is performing worse than random! Something is seriously wrong.\")\n", "    elif test_accuracy < 70:\n", "        print(\"\u26a0\ufe0f Model is learning but poorly. Some bugs remain.\")\n", "    elif test_accuracy < 85:\n", "        print(\"\ud83d\udc4d Model is learning reasonably. Maybe 1-2 bugs left.\")\n", "    else:\n", "        print(\"\u2705 Model is learning well! Most bugs are fixed.\")\n", "    \n", "    # Check prediction distribution\n", "    print(f\"\\nPrediction distribution:\")\n", "    print(f\"Training: {np.sum(y_pred_train == 0)} zeros, {np.sum(y_pred_train == 1)} ones\")\n", "    print(f\"Expected: ~{np.sum(y_train == 0)} zeros, ~{np.sum(y_train == 1)} ones\")\n", "    \n", "    if np.sum(y_pred_train == 0) == m_train or np.sum(y_pred_train == 1) == m_train:\n", "        print(\"\ud83d\udea8 All predictions are the same! Check your prediction function.\")\n", "    \n", "except Exception as e:\n", "    print(f\"\u274c Evaluation failed: {e}\")\n", "    print(\"Fix the bugs and try again!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "## \ud83c\udfaf DEBUG CHECKLIST\n", "\n", "Use this systematic approach to find all bugs:\n", "\n", "### 1. Data Validation\n", "- [ ] Check input shapes (X, y)\n", "- [ ] Check for NaN or Inf values\n", "- [ ] Verify labels are 0 or 1\n", "- [ ] Check data is properly normalized\n", "\n", "### 2. Initialization\n", "- [ ] Are weights initialized to reasonable values?\n", "- [ ] Is the initialization scale appropriate?\n", "- [ ] Check: print initial w statistics\n", "\n", "### 3. Forward Propagation\n", "- [ ] Is the sigmoid function numerically stable?\n", "- [ ] Are matrix dimensions correct?\n", "- [ ] Check: print A statistics (min, max, mean)\n", "\n", "### 4. Cost Function\n", "- [ ] Is the cost formula correct?\n", "- [ ] Are all signs correct?\n", "- [ ] Should cost increase or decrease?\n", "- [ ] Check: does cost match expected range?\n", "\n", "### 5. Backward Propagation\n", "- [ ] Are gradient formulas correct?\n", "- [ ] Are all signs correct (easy to flip!)?\n", "- [ ] Check: print gradient statistics\n", "- [ ] Verify: gradients should be small numbers\n", "\n", "### 6. Gradient Descent\n", "- [ ] Is learning rate appropriate?\n", "- [ ] Are we moving in the right direction?\n", "- [ ] Check: cost should decrease over time\n", "\n", "### 7. Prediction\n", "- [ ] Is the threshold correct (0.5)?\n", "- [ ] Is the comparison logic correct (>, <, >=, <=)?\n", "- [ ] Are predictions 0 and 1 (not reversed)?\n", "- [ ] Check: prediction distribution reasonable?\n", "\n", "---\n", "\n", "## \ud83d\udd2c DEBUGGING TOOLS\n", "\n", "Add these code snippets to investigate:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tool 1: Check for NaN or Inf\n", "def check_nan_inf(arr, name):\n", "    has_nan = np.any(np.isnan(arr))\n", "    has_inf = np.any(np.isinf(arr))\n", "    if has_nan or has_inf:\n", "        print(f\"\u26a0\ufe0f {name} contains NaN: {has_nan}, Inf: {has_inf}\")\n", "    return has_nan or has_inf\n", "\n", "# Example usage:\n", "# check_nan_inf(w, \"weights\")\n", "# check_nan_inf(A, \"activations\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tool 2: Print detailed statistics\n", "def print_stats(arr, name):\n", "    print(f\"\\n{name} statistics:\")\n", "    print(f\"  Shape: {arr.shape}\")\n", "    print(f\"  Min: {np.min(arr):.6f}\")\n", "    print(f\"  Max: {np.max(arr):.6f}\")\n", "    print(f\"  Mean: {np.mean(arr):.6f}\")\n", "    print(f\"  Std: {np.std(arr):.6f}\")\n", "    print(f\"  Contains NaN: {np.any(np.isnan(arr))}\")\n", "    print(f\"  Contains Inf: {np.any(np.isinf(arr))}\")\n", "\n", "# Example usage:\n", "# print_stats(w, \"weights\")\n", "# print_stats(grads[\"dw\"], \"gradients dw\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tool 3: Numerical gradient checking\n", "def check_gradient(w, b, X, y, epsilon=1e-7):\n", "    \"\"\"\n", "    Check if analytical gradient matches numerical gradient\n", "    \"\"\"\n", "    # Get analytical gradient\n", "    grads, _ = propagate(w, b, X, y)\n", "    \n", "    # Compute numerical gradient for first weight\n", "    w_plus = w.copy()\n", "    w_plus[0, 0] += epsilon\n", "    _, cost_plus = propagate(w_plus, b, X, y)\n", "    \n", "    w_minus = w.copy()\n", "    w_minus[0, 0] -= epsilon\n", "    _, cost_minus = propagate(w_minus, b, X, y)\n", "    \n", "    numerical_grad = (cost_plus - cost_minus) / (2 * epsilon)\n", "    analytical_grad = grads[\"dw\"][0, 0]\n", "    \n", "    print(f\"\\nGradient Check:\")\n", "    print(f\"Numerical gradient: {numerical_grad:.8f}\")\n", "    print(f\"Analytical gradient: {analytical_grad:.8f}\")\n", "    print(f\"Difference: {abs(numerical_grad - analytical_grad):.8f}\")\n", "    \n", "    if abs(numerical_grad - analytical_grad) < 1e-5:\n", "        print(\"\u2705 Gradients match!\")\n", "    else:\n", "        print(\"\u274c Gradients don't match - bug in gradient calculation!\")\n", "\n", "# Example usage:\n", "# w_test, b_test = initialize_parameters(n_features)\n", "# check_gradient(w_test, b_test, X_train[:, :10], y_train[:, :10])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "## \ud83d\udcdd BUG REPORT TEMPLATE\n", "\n", "Document each bug you find:\n", "\n", "### Bug #1:\n", "**Location:** (Function name, line)\n", "\n", "**What's wrong:** \n", "\n", "**Why it's wrong:** \n", "\n", "**How to fix:**\n", "\n", "**Observable symptoms:**\n", "\n", "---\n", "\n", "### Bug #2:\n", "**Location:** \n", "\n", "**What's wrong:** \n", "\n", "**Why it's wrong:** \n", "\n", "**How to fix:**\n", "\n", "**Observable symptoms:**\n", "\n", "---\n", "\n", "### Bug #3:\n", "**Location:** \n", "\n", "**What's wrong:** \n", "\n", "**Why it's wrong:** \n", "\n", "**How to fix:**\n", "\n", "**Observable symptoms:**\n", "\n", "---\n", "\n", "### Bug #4:\n", "**Location:** \n", "\n", "**What's wrong:** \n", "\n", "**Why it's wrong:** \n", "\n", "**How to fix:**\n", "\n", "**Observable symptoms:**\n", "\n", "---\n", "\n", "### Bug #5:\n", "**Location:** \n", "\n", "**What's wrong:** \n", "\n", "**Why it's wrong:** \n", "\n", "**How to fix:**\n", "\n", "**Observable symptoms:**\n", "\n", "---\n", "\n", "## \ud83c\udfc6 SCORING RUBRIC\n", "\n", "### Points by Bug Difficulty:\n", "- **Bug #1** (Sigmoid stability): 1 point - Medium difficulty\n", "- **Bug #2** (Initialization): 2 points - Easy to spot\n", "- **Bug #3** (Cost sign): 2 points - Requires understanding math\n", "- **Bug #4** (Gradient signs): 3 points - Hard, requires gradient understanding\n", "- **Bug #5** (Prediction logic): 2 points - Tricky logic error\n", "\n", "**Total: 10 points**\n", "\n", "### Time Bonuses:\n", "- Find all bugs in <15 min: +5 bonus points\n", "- Find all bugs in <30 min: +3 bonus points\n", "- Find all bugs in <45 min: +1 bonus point\n", "\n", "### Team Score:\n", "**Bug Points:** ___/10\n", "\n", "**Time Bonus:** ___/5\n", "\n", "**Explanation Quality:** ___/5 (instructor scores)\n", "\n", "**TOTAL:** ___/20\n", "\n", "---\n", "\n", "## \ud83d\udca1 HINTS (Use Sparingly!)\n", "\n", "<details>\n", "<summary>Hint 1: First thing to check (Click to reveal)</summary>\n", "\n", "Look at the initialization function. Are the weights a reasonable scale? Remember gradient descent works best with small initial values.\n", "</details>\n", "\n", "<details>\n", "<summary>Hint 2: Cost behavior (Click to reveal)</summary>\n", "\n", "If cost is INCREASING instead of decreasing, check:\n", "1. Are you minimizing or maximizing?\n", "2. Are gradient signs correct?\n", "3. Is learning rate too high?\n", "</details>\n", "\n", "<details>\n", "<summary>Hint 3: Predictions (Click to reveal)</summary>\n", "\n", "If all predictions are the same class, look at:\n", "1. The threshold comparison (>, <, >=, <=)\n", "2. Which class is assigned to which condition\n", "3. Are 0 and 1 swapped somewhere?\n", "</details>\n", "\n", "<details>\n", "<summary>Hint 4: Math check (Click to reveal)</summary>\n", "\n", "Cross-entropy loss formula:\n", "- Should have a NEGATIVE sign at the front\n", "- Gradients: dw = (1/m) * X * (A - Y).T\n", "- Double check all your signs!\n", "</details>\n", "\n", "<details>\n", "<summary>Hint 5: Numerical issues (Click to reveal)</summary>\n", "\n", "Sigmoid can overflow when z is very large (positive or negative). Consider clipping z or using a numerically stable sigmoid.\n", "</details>\n", "\n", "---\n", "\n", "**Good luck debugging! Remember: systematic debugging beats random guessing.** \ud83c\udfaf"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.12"}}, "nbformat": 4, "nbformat_minor": 4}